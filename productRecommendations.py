# -*- coding: utf-8 -*-
"""Recommendation System with sentiment scores & clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JX5icrRJPqIvLteD6id--MH6HUzcZDJr
"""

#import all the necessary packages.
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import pairwise_distances
from scipy.sparse import hstack
from sklearn.cluster import KMeans
from tqdm import tqdm
import pickle
import re
from scipy.sparse import hstack
import pickle
from sklearn.preprocessing import MinMaxScaler
import sys

data=pd.read_csv("C:\\Users\\harshita.chigati\\Desktop\\Python files\\NLP\\BigBasket Products.csv")
data.head(3)

print ('Number of data points : ', data.shape[0],'\nNumber of features/variables:', data.shape[1])

# each product/item has 10 features in the raw dataset.
data.columns # prints column-names or feature-names.

# info about the data features
data.info()

# to know null values in each column
data.isnull().sum()

# selecting the required columns from dataframe for analysis and model building
data=data[['product','category','sub_category','brand','type','description','sale_price','market_price']]
data.head(2)

# will remove rows with missing value in columns product,brand,description
data=data[~data['product'].isna()] # removing row with missing product value
data=data[~data['brand'].isna()]
data=data[~data['description'].isna()]
data.shape

# splitting data into train and test
from sklearn.model_selection import train_test_split

data_train,data_test=train_test_split(data,test_size=0.2,random_state=33)

print('data_train shape',data_train.shape)
print('data_test shape',data_test.shape)

# resetting index
data_train=data_train.reset_index(drop=True)
data_test=data_test.reset_index(drop=True)

"""## EDA on Train data"""

data_train.columns

# Univariate Analysis for feature 'product'
data_train['product'].describe()

# Univariate Analysis For feature category
data_train['category'].describe()

d1=data_train['category'].value_counts().reset_index()
d1.columns=['category','counts']
d1

# calculating the percentage distribution of the 'category' column
cate=data_train['category'].value_counts(normalize=True).reset_index() # creating data frame with name cate for further processing
cate['cumulative%']= cate['category'].cumsum()
cate.columns=['category','%','cumulative%']
cate

plt.figure(figsize=(12,5))
ax=sns.barplot(x='category',y='counts',data=d1,palette="Blues_d")
ax.set_xticklabels(ax.get_xticklabels(), rotation=40,ha="right")
for bar in ax.patches: # annotating bar graph
    ax.annotate(format(bar.get_height(), '.0f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=8, xytext=(0, 8),
                   textcoords='offset points')

ax2=ax.twinx() # to include line graph with bar graph in same plot
ax2=sns.lineplot(x='category',y='cumulative%',data=cate,marker="o",color='r')

for i,value in enumerate(cate['cumulative%'].values): # to annotate line plot
    ax2.text(cate['category'].values[i],cate['cumulative%'].values[i]-0.0025,np.round(value*100,2))
    # putting text of cum_% value with respect to its x,y values
    #ax2,text(x-coordinate,ycordinate,value) # position of annotation
ax.set_title('Univariate Analysis of Category') # title the graph
plt.tight_layout()
plt.show()

# For feature 'sub_category'
data_train['sub_category'].describe()

data_train['sub_category'].value_counts()

data_train['sub_category'].value_counts(normalize=True)*100 # To get percentage

data_train['sub_category'].unique() # to get unique values of sub_category

#  top 10 sub_categories
subcategory_count=Counter(list(data_train['sub_category']))
subcategory_count.most_common(10)

d1=data_train['sub_category'].value_counts().reset_index()[0:10] # selecting top 10 sub categories
d1.columns=['sub_category','counts'] # renaming colimns
d1

plt.figure(figsize=(8,4))
ax=sns.barplot(x='sub_category',y='counts',data=d1,palette="Accent_d")
ax.set_xticklabels(ax.get_xticklabels(), rotation=40,ha="right")

# annotating values
for bar in ax.patches: # annotating bar graph
    ax.annotate(format(bar.get_height(), '.0f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=8, xytext=(0, 3.5),
                   textcoords='offset points')
ax.set_title('Univariate Analysis of Sub_Category') # title the graph

plt.show()

# For Feature 'Brand'
data_train['brand'].describe()

#  top 10 brand
brand_count=Counter(list(data_train['brand']))
brand_count.most_common(10)

# percentage of top 10 brands
(data_train['brand'].value_counts(normalize=True)*100)[:10]

# For Feature 'type'
data_train['type'].describe()

#  top 10 type of products
type_count=Counter(list(data_train['type']))
type_count.most_common(10)

# percentage of top 10 types
(data_train['type'].value_counts(normalize=True)*100)[:10]

"""### sales_price"""

#  top sale_price
saleprice_count=Counter(list(data_train['sale_price']))
saleprice_count.most_common(10)

data_train['sale_price'].value_counts().sort_index(ascending=False)

#CDF of sale_price
sns.set(rc={'figure.figsize':(10,5)})
sns.set_style('whitegrid')
sns.ecdfplot(data_train['sale_price'])

# Univariate analysis of sale_price
#Plotting of sale_price variable
sns.set(rc={'figure.figsize':(10,5)})
sns.set_style('white')
sns.boxplot(y=data_train['sale_price'])

ypoints=np.array(data_train['sale_price'].sort_values())
plt.plot(ypoints,'r-')
plt.ylabel('sale_price')
plt.grid()
plt.show()

# data with sale price greater than
data_saleprice=data_train[data_train['sale_price']>2000]
data_saleprice.shape

data_train=data_train[data_train['sale_price']<=2000]
print("No. of data points after removing sale_price outliers:",data_train.shape[0])

"""### Description feature"""

data_train['description'].value_counts()

data_train['description'].isna().sum() # to check for any missing values

data_train[data_train['description']==u' '].any().sum() # to check for just space in description

"""## Data Preprocessing

"""

# we use the list of stop words that are downloaded from nltk lib.
import nltk # to download only once
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
print ('list of stop words:', stop_words)

"""### Categorical features"""

# preprocressing category feature
def preprocess_categorical(data,column_name):
    """ fuction to preprocess categorical data column"""

    data[column_name]=data[column_name].str.replace('&','_') # replacing & with _
    data[column_name]=data[column_name].str.replace(',','_') # replacing , with _
    data[column_name]=data[column_name].str.replace("'",'') #replacing ' with ''(no space)
    data[column_name]=data[column_name].str.replace(" ",'') # removing white spaces
    data[column_name]=data[column_name].str.lower() # to lower case
    data[column_name]=data[column_name].str.strip() # removing trailing and leading white space

    return data[column_name]

data_train['category']=preprocess_categorical(data_train,'category')
data_train['category'].unique() # to check

data_train['sub_category']=preprocess_categorical(data_train,'sub_category')
data_train['sub_category'].unique()

data_train['brand']=preprocess_categorical(data_train,'brand')
data_train['brand'].nunique()

data_train['type']=preprocess_categorical(data_train,'type')
data_train['type'].nunique()

from tqdm import tqdm
import re
def preprocess_description(text):
    preprocessed_description=[]
    for description in tqdm(text):

        #Delete all the data which are present in the brackets
        description = re.sub(r'\([^()]*\)',' ',description)

        #removing urls
        description = re.sub(r'http\S+',' ',description)
        description = re.sub('[^A-Za-z]+', ' ', description) # remove all characters except a-z and A-Z and replace with white space
        # https://gist.github.com/sebleier/554280
        description = ' '.join(word for word in description.split() if word.lower() not in stop_words) # removing stop words
        description = ' '.join(word for word in description.split() if len(word)>2) # removing single letter and two letter words
        description = description.lower().strip()
        preprocessed_description.append(description)

    return preprocessed_description
preprocessed_description=preprocess_description(data_train['description'].values)

data_train['description']=preprocessed_description
data_train

print('Printing some random description values')
print('-'*50)
print('[9]',data_train['description'][9])
print('-'*50)
print('[288]',data_train['description'][288])
print('-'*50)
print('[457]',data_train['description'][457])
print('-'*50)

"""### Cluster Analysis"""

# computing sentiment score for description feature
import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from tqdm import tqdm
sia=SentimentIntensityAnalyzer()

# saving sentiment scores in list
negative=[]
neu=[]
pos=[]
compound=[]

for description in tqdm(data_train['description']):
    i=sia.polarity_scores(description)['neg']
    j=sia.polarity_scores(description)['neu']
    k=sia.polarity_scores(description)['pos']
    l=sia.polarity_scores(description)['compound']
    negative.append(i)
    neu.append(j)
    pos.append(k)
    compound.append(l)

# adding sentiment intensity analyser scores to dataframe as columns
data_train['negative']=negative
data_train['neutral']=neu
data_train['positive']=pos
data_train['compound']=compound

data_train['sale_price'].describe()

sale_price_min=data_train['sale_price'].min()
sale_price_min

sale_price_max=data_train['sale_price'].max()
sale_price_max

# scaling the sale price using min max scaler and aading it as new column to dataframe
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
data_train['sale_price_scaled']=scaler.fit_transform(data_train['sale_price'].values.reshape(-1,1))

data_train.head()

data_train[['sale_price_scaled','negative','neutral','positive','compound']].describe()

# concatenating sentiment scores of description,sale_price_scaled,discount_% for clustering analysis
# using only numerical features for getting means(centroids) of cluster for further assignment
X_train = np.hstack((data_train['sale_price_scaled'].values.reshape(-1,1),\
           data_train['negative'].values.reshape(-1,1),data_train['neutral'].values.reshape(-1,1), \
           data_train['positive'].values.reshape(-1,1),data_train['compound'].values.reshape(-1,1)))

X_train.shape

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # performing clustering using Kmeans
# from sklearn.cluster import KMeans
# from sklearn.metrics import silhouette_score # to evaluate clustering performance
# silhouette_scr=[]
# K=range(2,41)
# for k in K:
#     kmeans=KMeans(n_clusters=k,n_init=12,max_iter=200) # setting KMeans cluster with 12 different centriod initializations & 200 iterations for a single run
#     kmeans=kmeans.fit(X_train)
#     labels=kmeans.labels_
# 
#     # silhouette score
#     silhouette_scr.append(silhouette_score(X_train,labels))
# 
# plt.plot(K,silhouette_scr,'b*-')
# plt.xlabel('K: No.of Clusters')
# plt.ylabel('Silhouette score')
# plt.title('Silhouette analysis For Optimal k')
# plt.show()

# will group data into 5 clusters
best_k=5
kmeans=KMeans(n_clusters=best_k,n_init=12,max_iter=200) # setting KMeans cluster with 12 different centriod initializations & 200 iterations for a single run
kmeans=kmeans.fit(X_train)

labels=kmeans.labels_ # labels for each product given by clustering model
len(labels)

# getting centroids
cluster_centers=kmeans.cluster_centers_
cluster_centers

#saving cluster centers to pkl file
pickle.dump(cluster_centers,open('cluster_centers.pkl','wb'))

# inserting cluster_label as column
data_train['cluster_label']=labels
data_train.head()

#saving preprocessed train data to csv
data_train.to_csv('train_preprocessed_with_clusterlabels.csv',index=False)

# saving raw test data to csv
data_test.to_csv('test_raw.csv',index=False)

# load preprocessed train data
df=pd.read_csv('train_preprocessed_with_clusterlabels.csv')
df.head()

df.columns
df.shape

import os

# Path to the GloVe text file
glove_file_path = r'C:\Users\harshita.chigati\Desktop\Python files\NLP\glove_data\glove.6B.50d.txt'

# Check if the file exists
if os.path.isfile(glove_file_path):
    # Read the GloVe file and process its contents
    glove_words = set()
    with open(glove_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            word_vector = line.split()
            word = word_vector[0]  # Extract the word
            glove_words.add(word)
else:
    print(f"The GloVe file '{glove_file_path}' does not exist.")

# Now glove_words contains the set of words in the GloVe file

# Check if the file exists
if os.path.isfile(glove_file_path):
    # Create a dictionary to store word vectors
    glove_vectors = {}
    with open(glove_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = list(map(float, values[1:]))
            glove_vectors[word] = vector
else:
    print(f"The GloVe file '{glove_file_path}' does not exist.")

# Now glove_vectors is a dictionary where keys are words and values are their corresponding vectors

"""### NER"""

import pandas as pd
import spacy

# Load SpaCy English language model (large)
nlp = spacy.load("en_core_web_lg")

# Function to perform NER on description
def perform_ner(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Apply NER to description column
df['ner_entities'] = df['description'].apply(perform_ner)

# Display the DataFrame with NER entities
print(df[['description', 'ner_entities']])

print(df)

# Flatten the list of entities and get the second element of each tuple (which represents the entity group)
flat_entity_groups = [entity[1] for sublist in df['ner_entities'] for entity in sublist]

# Get unique entity groups
unique_entity_groups = set(flat_entity_groups)

# Display the unique entity groups
print("Unique entity groups:", unique_entity_groups)

# Create separate columns for each entity group
for entity_group in unique_entity_groups:
    column_name = f"{entity_group.lower()}_entities"  # New column name based on entity group
    df[column_name] = df['ner_entities'].apply(lambda x: [entity[0] for entity in x if entity[1] == entity_group])

# Display the DataFrame with separate columns for each entity group
print(df[['product', 'category', 'sub_category', 'brand', 'sale_price', 'market_price', 'type', 'description', 'ner_entities'] + [f"{entity_group.lower()}_entities" for entity_group in unique_entity_groups]])

from spacy.lang.en.stop_words import STOP_WORDS

# Function to process text
def process_entities(entities):
    processed_entities = []
    for entity in entities:
        # Tokenization and lemmatization
        doc = nlp(entity)
        tokens = [token.lemma_ for token in doc]
        # Stopword removal
        tokens = [token for token in tokens if token.lower() not in STOP_WORDS]
        processed_entities.extend(tokens)
    return processed_entities

# Process entities columns
for entity_group in unique_entity_groups:
    column_name = f"{entity_group.lower()}_entities"
    df[column_name] = df[column_name].apply(lambda x: process_entities(x) if isinstance(x, list) else [])

# Display the DataFrame with processed entities columns
print(df[['product', 'category', 'sub_category', 'brand', 'sale_price', 'market_price', 'type', 'description', 'ner_entities'] + [f"{entity_group.lower()}_entities" for entity_group in unique_entity_groups]])

# Remove brackets and quotes from the values in entity columns
for entity_group in unique_entity_groups:
    column_name = f"{entity_group.lower()}_entities"
    df[column_name] = df[column_name].apply(lambda x: x[0] if len(x) > 0 else None)

# Display the DataFrame with separate columns for each entity group
print(df[['product', 'category', 'sub_category', 'brand', 'sale_price', 'market_price', 'type', 'description', 'ner_entities'] + [f"{entity_group.lower()}_entities" for entity_group in unique_entity_groups]])

# Remove the ner_entities column from df
df = df.drop(columns=['ner_entities'])

# Display the DataFrame with processed entities columns and without the ner_entities column
print(df)

df.dtypes

# tfidf vectorizer
tfidf=TfidfVectorizer()
tfidf_description=tfidf.fit_transform(df['description'])

df.dtypes

# we are converting a dictionary with word as a key, and the idf as a value
dictionary = dict(zip(tfidf.get_feature_names_out(), list(tfidf.idf_)))
tfidf_words = set(tfidf.get_feature_names_out())

df1 = df.copy()

df1.head()

"""### TF-IDF Vectorization"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm

# Fill missing values with empty strings in entity group columns
entity_group_columns = ['org_entities', 'product_entities', 'loc_entities', 'ordinal_entities', 'date_entities', 'person_entities', 'norp_entities', 'cardinal_entities', 'gpe_entities', 'time_entities', 'fac_entities']
df1[entity_group_columns] = df1[entity_group_columns].fillna('')

# Concatenate product, category, and subcategory columns with the description column
df1['text'] = df1['description'] + " " + df1['org_entities'] + " " + df1['product_entities'] + " " + df1['loc_entities'] + " " + df1['ordinal_entities'] + " " + df1['date_entities'] + " " + df1['person_entities'] + " " + df1['norp_entities'] + " " + df1['cardinal_entities'] + " " + df1['gpe_entities'] + " " + df1['time_entities'] + " " + df1['fac_entities']+ " " + " " + df1['category'] + " " + df1['sub_category'] + df1['brand'] + df1['type']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the text data to obtain TF-IDF vectors
tfidf_matrix = tfidf_vectorizer.fit_transform(df1['text'])

# Display the shape of the TF-IDF matrix
print("Shape of TF-IDF matrix:", tfidf_matrix.shape)

# Calculate TF-IDF weighted GloVe vectors for each description
description_tfidf_w2v_vectors = []
for description in tqdm(df['description']): # for each description
    vectors = [] # store GloVe vectors for words in the description
    tf_idf_weights = [] # store TF-IDF weights for words in the description
    for word in description.split(): # for each word in description
        if (word in glove_vectors) and (word in tfidf_vectorizer.vocabulary_):
            vec = np.array(glove_vectors[word]) # getting the vector for each word
            tf_idf_index = tfidf_vectorizer.vocabulary_[word] # get the index of the word in the TF-IDF matrix
            tf_idf = tfidf_matrix[0, tf_idf_index] # get the TF-IDF value for the word
            vectors.append(vec)
            tf_idf_weights.append(tf_idf)
    if vectors:
        vectors = np.array(vectors)
        tf_idf_weights = np.array(tf_idf_weights)
        weighted_vectors = vectors * tf_idf_weights[:, np.newaxis] # apply TF-IDF weights to GloVe vectors
        avg_vector = np.mean(weighted_vectors, axis=0) # compute the average weighted GloVe vector
        description_tfidf_w2v_vectors.append(avg_vector)
    else:
        description_tfidf_w2v_vectors.append(np.zeros(300)) # handle case when no valid GloVe vectors are found


# Step 6: Store TF-IDF weighted Word2Vec vectors
df1['description_tfidf_w2v'] = description_tfidf_w2v_vectors

# stacking all encoded categorical features, vectorized description text and scaled sael price,discount% and sentiment scores
X_train=np.hstack((description_tfidf_w2v_vectors,df1['category'].values.reshape(-1,1),df1['sub_category'].values.reshape(-1,1),df1['brand'].values.reshape(-1,1), \
                  df1['type'].values.reshape(-1,1),df1['sale_price_scaled'].values.reshape(-1,1), \
                  df1['negative'].values.reshape(-1,1),df1['neutral'].values.reshape(-1,1),df1['positive'].values.reshape(-1,1), \
                  df1['compound'].values.reshape(-1,1),df1['cluster_label'].values.reshape(-1,1)))

X_train.shape

# function to get sentiment scores
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def get_scores(data):
    """returns sentiment analysis scores for description feature"""
    sia=SentimentIntensityAnalyzer()

    negative=[]
    neu=[]
    pos=[]
    compound=[]
    if 'description' in data.columns:
        for value in (data['description']):
            i=sia.polarity_scores(value)['neg']
            j=sia.polarity_scores(value)['neu']
            k=sia.polarity_scores(value)['pos']
            l=sia.polarity_scores(value)['compound']

            negative.append(i)
            neu.append(j)
            pos.append(k)
            compound.append(l)

    data['negative']=negative
    data['neutral']=neu
    data['positive']=pos
    data['compound']=compound

    return data

import numpy as np
import sys

def get_clusterlabel(X, means=cluster_centers):
    """Compute cluster label based on the nearest mean."""
    minimum = sys.maxsize
    index = -1

    for i in range(len(means)):
        if isinstance(X, list):
            X_array = np.array(X)
        else:
            X_array = X.values.reshape(1, -1)

        sum_squared_diff = 0
        for j in range(len(X_array)):
            diff = X_array[j] - means[i][j]
            sum_squared_diff += diff * diff

        euclidean_dist = np.sqrt(sum_squared_diff)

        if euclidean_dist < minimum:
            minimum = euclidean_dist
            index = i

    return index

"""### Recommending products"""

from sklearn.metrics.pairwise import linear_kernel

def get_similar_products(df1, user_input, X_train=X_train, num_results=5):
    # Convert user_input to string if it's not already
    user_input = str(user_input)

    # Filter DataFrame based on user input (category, subcategory, type, or brand)
    filtered_df = df1[(df1['category'].str.contains(user_input)) |
                      (df1['sub_category'].str.contains(user_input)) |
                      (df1['brand'] == user_input) |
                      (df1['type'] == user_input)]

    if filtered_df.empty:
        return "No products found for the specified input."

    # Compute cosine similarities using linear kernel
    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

    # Step 2: Compute average similarity scores for filtered products
    avg_similarity_scores = cosine_sim[filtered_df.index].mean(axis=0)

    # Step 3: Get indices of top 5 products with highest average similarity scores
    top_5_indices = avg_similarity_scores.argsort()[::-1][:5]

    # Step 4: Get product titles of top 5 recommended products within the filtered category, subcategory, or brand
    top_5_products = df1.iloc[top_5_indices]['product'].tolist()

    return top_5_products

# Prompt the user to enter their input
user_input = input("Enter category, subcategory, brand, or type: ")

# Call the function with the user input
similar_products = get_similar_products(df1, user_input, X_train, num_results=5)

# Print the results
print("Top 5 similar products:")
for i, product in enumerate(similar_products, start=1):
    print(f"{i}. {product}")

from sklearn.preprocessing import MinMaxScaler
import sys
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import linear_kernel

def get_similar_products_1(df1, user_input, X_train=X_train, num_results=5):
    # Convert user_input to string if it's not already
    user_input = str(user_input)

    # Filter DataFrame based on user input (category, subcategory, type, or brand)
    filtered_df = df1[(df1['category'].str.contains(user_input)) |
                      (df1['sub_category'].str.contains(user_input)) |
                      (df1['brand'] == user_input) |
                      (df1['type'] == user_input)]

    if filtered_df.empty:
        return "No products found for the specified input."

    # Compute cosine similarities using linear kernel
    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

    # Step 2: Compute average similarity scores for filtered products
    avg_similarity_scores = cosine_sim[filtered_df.index].mean(axis=0)

    # Step 3: Get indices of top 10 products with highest average similarity scores
    top_10_indices = avg_similarity_scores.argsort()[::-1][:10]

    # Step 4: Get product titles of top 10 recommended products within the filtered category, subcategory, or brand
    top_10_products = df1.iloc[top_10_indices]['product'].tolist()

    return top_10_products

# Prompt the user to enter their input
user_input = input("Enter category, subcategory, brand, or type: ")

# Get the top 10 products based on user input
top_10_products = get_similar_products_1(df1, user_input, X_train, num_results=10)

# Remove duplicates from the top 10 products list
unique_top_10_products = list(set(top_10_products))

# Print the top 10 products
print("Top 10 products based on user input:")
for i, product in enumerate(unique_top_10_products, start=1):
    print(f"{i}. {product}")

# Apply sentiment score analysis to get the top 5 unique products
top_10_df = df1[df1['product'].isin(unique_top_10_products)]  # Subset DataFrame to include only unique top 10 products
top_10_df_with_scores = get_scores(top_10_df)  # Apply sentiment score analysis to top 10 products

# Sort top 10 products by compound sentiment score
top_10_df_sorted = top_10_df_with_scores.sort_values(by='compound', ascending=False)

# Get the final top 5 unique products after sorting by sentiment score
top_5_products = top_10_df_sorted['product'].unique()[:5]

# Print the top 5 unique products after sentiment score analysis
print("\nTop 5 unique products after sentiment score analysis:")
for i, product in enumerate(top_5_products, start=1):
    print(f"{i}. {product}")